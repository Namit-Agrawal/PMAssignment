---
title: "Exercise"
author: "Namit Agrawal, Timothy Cheng"
date: "08/16/20"
output:
  pdf_document: default
---
Note: Much of the analysis below was inspired by various R files written by Professor James Scott.  
# **Problem 1: Visual story telling: green buildings**
```{r include=FALSE}
library(mosaic)
library(ggplot2)
library(reshape)
library(chron)
library(quantmod)
library(tidyverse)
```

```{r include=FALSE}
greenBuild = read.csv("greenbuildings.csv", stringsAsFactors = FALSE)
greenBuild$green_rating = as.factor(greenBuild$green_rating)
greenBuild$class_a = as.factor(greenBuild$class_a)
greenBuild$renovated = as.factor(greenBuild$renovated)
greenBuild$amenities = as.factor(greenBuild$amenities)

attach(greenBuild)
```

It does make sense to use the median rather than mean as the non-green buildings
have many outliers as suggested by the boxplot below
```{r echo=FALSE, fig.align = "center",fig.width = 6,fig.height = 4}
ggplot(greenBuild, aes(x=green_rating, y=Rent)) + 
  geom_boxplot()
```


The stats guru is right about removing buildings with less than 10%  occupancy
as based on the summary below. Within the group of buildings that have only 10% occupancy, there is only one building with a green rating.  In addition, roughly half of the buildings have 3 stories and very few buildings have a Class A designation.  Hence
we should remove buildings with less than 10% occupancy as it may distort the analysis.
```{r echo=FALSE}
summary(subset(greenBuild, leasing_rate<10))
```

The median rent for green buildings and non-green buildings is correct if buildings
with more than 10% occupancy rate are considered.
```{r echo=FALSE}
greenBuild90 = subset(greenBuild, leasing_rate>=10)
medianRent = greenBuild90 %>% 
  group_by(green_rating)  %>%  
  summarize(MedianRent = median(Rent, na.rm = TRUE)) 
medianRent
```
Assuming that the building is 250000 square feet, it seems that the stats guru is correct about
recuperating the costs in a little under 8 years.

**Confounding variables investigation**\

**Renovated Buildings**\
Based on the numerical summary analysis for buildings that are renovated,
it does not seem there is much confounding going on as the median rents 
for non renovated and renovated buildings are similar especially for green buildings

```{r echo=FALSE}
renovated_group = greenBuild90 %>%
  group_by(renovated, green_rating)  %>%  
  summarize(MedianRent = median(Rent, na.rm = TRUE)) 
renovated_group
```

```{r echo=FALSE}
renovated_group_rent = greenBuild90 %>%
  group_by(renovated)  %>%  
  summarize(MedianRent = median(Rent, na.rm = TRUE)) 
renovated_group_rent
```

**Number of Stories**\
As suggested by the plot below, the median for stories is a valid selection since there are some outliers.
It looks there is not much evidence of confounding for the number of stories in the building, even though there is a slight increase in rent as the number of stories goes up. The median for stories of green buildings only differs by 1, so stories may not directly be affecting the rent.

```{r echo=FALSE,fig.align = "center",fig.width = 6,fig.height = 4}
ggplot(data=greenBuild90) + 
  geom_point(mapping=aes(x=stories, y=Rent, colour=green_rating)) +
  labs(x="Stories", y='Rent', title = 'Green buildings: Stories VS Rent',
       color='Green building')
```

```{r echo=FALSE}
stories_group = greenBuild90 %>%
  group_by(green_rating)  %>%  
  summarize(MedianStories = median(stories, na.rm = TRUE)) 
stories_group
```

**Age**\

An initial analysis provides a stark contrast in age between green and non-green buildings.
```{r,echo = FALSE,fig.align = "center",fig.width = 6,fig.height = 4}
ggplot(greenBuild, aes(x=green_rating, y=age)) + 
  geom_boxplot()
```
However, it looks like there is no confounding for age, as there is no correlation between
the age of the building and the rent from the plot below.
```{r echo=FALSE,fig.align = "center",fig.width = 6,fig.height = 4}
ggplot(data=greenBuild90) + 
  geom_point(mapping=aes(x=age, y=Rent, colour=green_rating)) +
  labs(x="Age", y='Rent', title = 'Green buildings: Age VS Rent',
       color='Green building')
```

```{r echo=FALSE}
age_group = greenBuild90 %>%
  group_by(green_rating)  %>%  
  summarize(MedianAge = median(age, na.rm = TRUE)) 
age_group
```

**Size**\
It looks like size is definitely a confounding variable, as size is correlated 
with rent from plot below and the median size for green buildings is double that of non-green. Thus, there is a premium in rent for larger sizes, as expected.
```{r echo=FALSE}
size_group = greenBuild90 %>%
  group_by(green_rating)  %>%  
  summarize(MedianSize = median(size, na.rm = TRUE)) 
size_group
```

```{r echo=FALSE,fig.align = "center",fig.width = 6,fig.height = 4}
ggplot(data=greenBuild90) + 
  geom_point(mapping=aes(x=size, y=Rent, colour=green_rating)) +
  labs(x="Size", y='Rent', title = 'Green buildings: size VS Rent',
       color='Green building')
```

**Cluster Rent**\
There does not seem to be confounding for cluster rent, as the median for cluster 
rent is approximately the same between green and non-green buildings. However, cluster rent is highly
correlated with the rent of the building.
```{r echo=FALSE}
cluster_rent_group = greenBuild90 %>%
  group_by(green_rating)  %>%  
  summarize(MedianClusterRent = median(cluster_rent, na.rm = TRUE)) 
cluster_rent_group
```

```{r echo=FALSE,fig.align = "center",fig.width = 6,fig.height = 4}
ggplot(data=greenBuild90) + 
  geom_point(mapping=aes(x=cluster_rent, y=Rent, colour=green_rating)) +
  labs(x="Cluster Rent", y='Rent', title = 'Green buildings: Cluster rent VS Rent',
       color='Green building')
```

**Class**\
It looks like the Class designation of buildings is a confounding variable, as Class A buildings
have generally higher median rents - in addition, having a green_rating with the Class A designation drives median rents even higher. Class A designated buildings seem to correlate with overall rent as
well.
```{r echo=FALSE}
classa_group= greenBuild90 %>%
  group_by(class_a, green_rating)  %>%  
  summarize(MedianRent = median(Rent, na.rm = TRUE)) 
classa_group
```

```{r,echo = FALSE,fig.align = "center",fig.width = 6,fig.height = 4}
ggplot(greenBuild, aes(x=class_a, y=Rent)) + 
  geom_boxplot()
```

```{r echo=FALSE}
classa_group_rent= greenBuild90 %>%
  group_by(class_a)  %>%  
  summarize(MedianRent = median(Rent, na.rm = TRUE)) 
classa_group_rent
```

**Thoughts**\
From the investigation above it seems like size and class are the only
confounding variables, as the green buildings tend to have larger spaces
and larger spaces have higher rent. However, the stats guru is only taking into account
the median rent of all the building with more than 10% occupancy.  If we apply another filter to include only 15 story buildings, we see that the rent goes up 
drastically for green buildings - all the way to 37 dollars. However, it may not
be wise to use this filter as there is only 10 green buildings that have 15 stories.
```{r echo=FALSE}
greenBuilding9015= subset(greenBuild90, stories==15)
stories_15_group = greenBuilding9015 %>%
  group_by(green_rating)  %>%  
  summarize(MedianRent = median(Rent, na.rm = TRUE), num=length(Rent)) 
stories_15_group
```

In general, the guru is correct with his analysis, but the analysis is performed on a dataset with a large range of different building specifications.  For example, the dataset only contains 10 green buildings that have 15 stories.  A larger sample size that adheres to the developers desired specs would provide more valid results.\

# **Problem 2: Visual story telling: flights at ABIA**

**Most active airlines**\
```{r include=FALSE}
abia = read.csv("abia.csv", stringsAsFactors = FALSE)
abia$Month = as.factor(abia$Month)
abia$DayOfWeek = as.factor(abia$DayOfWeek)


abia$DepHour=sapply(abia$DepTime, function(x) x%/%100)
abia$ArrHour=sapply(abia$ArrTime, function(x) x%/%100)

abia$Diff_DepTime=difftime(as.POSIXct((times=sub("(\\d+)(\\d{2})", "\\1:\\2", abia$DepTime)),
           format='%H:%M'),  as.POSIXct((times=sub("(\\d+)(\\d{2})", "\\1:\\2", abia$CRSDepTime)),
                                        format='%H:%M'), units="mins")
abia$Diff_DepTime=as.numeric(abia$Diff_DepTime)
abia_austin_Dep = subset(abia, Origin == 'AUS')
abia_austin_Arr = subset(abia, Dest == 'AUS')

attach(abia)
```
Here we analyze which airlines are most active throughout the year in terms of
the distance flown. As seen by the plot, it appears Southwest (WN) and American
Airlines(AA) are the most active in flying out of ABIA and flying to ABIA.
```{r echo=FALSE, fig.align = "center",fig.width = 6,fig.height = 4}
ggplot(abia, aes(x=Month, y=Distance)) + 
  geom_bar(stat='identity') +geom_bar(stat='identity') +
  facet_wrap(~ UniqueCarrier, nrow = 4)+
  scale_x_discrete(guide = guide_axis(check.overlap = TRUE))+
  labs(title="Distance traveled by different airlines each month", 
       y="Distance",
       x = "Month")+
  theme(plot.title = element_text(hjust = 0.5))
```

**Delays by Day of Week**\
```{r include=FALSE}
delay_dep_sum = abia_austin_Dep %>%
  group_by(DayOfWeek)  %>%  
  summarize(ToDepDelay = sum(DepDelay, na.rm = TRUE)) 

delay_arr_sum = abia_austin_Arr %>%
  group_by(DayOfWeek)  %>%  
  summarize(ToArrDelay = sum(ArrDelay, na.rm = TRUE)) 
delay_sum = merge(delay_dep_sum, delay_arr_sum, by="DayOfWeek")
delay_sum <- melt(delay_sum, id.vars = 'DayOfWeek')

```
In the plot below, we analyze the departure and arrival delay for each day of the week.
There are more arrival delays than departure delays and Friday is the worst day to travel to/from Austin.
```{r echo=FALSE, fig.align = "center",fig.width = 6,fig.height = 4}
ggplot(delay_sum, aes(x=DayOfWeek, y=value, fill=variable )) + 
  geom_bar(stat='identity', position='dodge') +
  labs(title="Departure and arrival delay for each day of week", 
       y="Total Delay Time",
       x = "Day Of the Week (1 = Monday)",
       fill="Delay")+
  theme(plot.title = element_text(hjust = 0.5))
```

**Average Delays per Airline**\
```{r include=FALSE}
num_dep_delay_by_carrier = abia_austin_Dep %>%
  group_by(UniqueCarrier)  %>%  
  summarize(AvgDepDelay = mean(DepDelay, na.rm = TRUE)) 

num_arr_delay_by_carrier = abia_austin_Arr %>%
  group_by(UniqueCarrier)  %>%  
  summarize(AvgArrDelay = mean(ArrDelay, na.rm = TRUE)) 

num_delay_by_carrier = merge(num_dep_delay_by_carrier, num_arr_delay_by_carrier, by="UniqueCarrier")
num_delay_by_carrier = as.data.frame(num_delay_by_carrier)
num_delay_by_carrier <- melt(num_delay_by_carrier, id.vars = 'UniqueCarrier')
```
This plot analyzes the average departure and arrival delay for each airline.  It looks like Piedmont Airlines (US) arrives and departs early on average as the delay time
is negative.
```{r echo=FALSE, fig.align = "center",fig.width = 6,fig.height = 4}
ggplot(num_delay_by_carrier, aes(x=UniqueCarrier, y=value, fill=variable )) + 
  geom_bar(stat='identity', position='dodge') +
  labs(title="Average departure and arrival delay for each airline", 
       y="Average Delay Time",
       x = "Airline Carrier Code",
       fill="Delay")+
  theme(plot.title = element_text(hjust = 0.5))

```

**Most common Delay types by Airline**\
```{r include=FALSE}
num_cancelled_by_carrier = abia %>%
  group_by(UniqueCarrier, CancellationCode)  %>%  
  summarize(Cancelprob = sum(Cancelled, na.rm = TRUE))

num_cancelled_dummy = abia %>%
  group_by(UniqueCarrier)  %>%  
  summarize(Total = length(Cancelled))
Cancellation_probs=merge(num_cancelled_by_carrier, num_cancelled_dummy, by="UniqueCarrier")
Cancellation_probs$True_prob = Cancellation_probs$Cancelprob/Cancellation_probs$Total
Cancellation_probs = subset(Cancellation_probs, CancellationCode=='A' | CancellationCode=='B' | CancellationCode=='C' )
Cancellation_probs$CancellationCode = ifelse(Cancellation_probs$CancellationCode=='A', "Carrier", ifelse(Cancellation_probs$CancellationCode =="B", "Weather", "NAS"))
```

Here we analyze the proportions of delays by airlines via the type of delay.
It look like most airlines suffer from carrier and weather delays.
```{r echo=FALSE, fig.align = "center",fig.width = 6,fig.height = 4}
ggplot(Cancellation_probs, aes(x=UniqueCarrier, y=True_prob, fill=CancellationCode )) + 
  geom_bar(stat='identity', position='dodge')+
  labs(title="Proportion of delay for each airline by type of delay", 
       y="Proportion",
       x = "Airline Carrier Code",
       fill="Type of Delay")+
  theme(plot.title = element_text(hjust = 0.5))
```

**Flights per hour of the Day**\
```{r include=FALSE}
num_flights_dep_per_hour = abia_austin_Dep %>%
  group_by(DepHour)  %>%  
  summarize(TotalDep = length(DepHour))

num_flights_arr_per_hour = abia_austin_Arr %>%
  group_by(ArrHour)  %>%  
  summarize(TotalArr = length(ArrHour))

num_flights_arr_per_hour=subset(num_flights_arr_per_hour, !is.na(ArrHour))
num_flights_dep_per_hour=subset(num_flights_dep_per_hour, !is.na(DepHour))
colnames(num_flights_arr_per_hour)[1] <- "Hour"
colnames(num_flights_dep_per_hour)[1] <- "Hour"

num_flights_per_hours = merge(num_flights_arr_per_hour, num_flights_dep_per_hour, by="Hour")
num_flights_per_hours <- melt(num_flights_per_hours, id.vars = 'Hour')
```

The plot below analyzes what are most frequent departure and arrival times.  Passengers typically fly *out* early in the morning and fly *in* late at night. Between
noon and evening there is an even split between passengers flying in and out.
```{r echo=FALSE, fig.align = "center",fig.width = 7,fig.height = 4}
ggplot(num_flights_per_hours, aes(x=Hour, y=value, fill=variable )) + 
  geom_bar(stat='identity', position='dodge') +
  labs(title="Number of flights at ABIA for each hour by departure and arrival", 
       y="Number of Flights",
       x = "Hour",
       fill="Departure-Arrival")+
  theme(plot.title = element_text(hjust = 0.5))
```

**Flights by City**\
```{r include=FALSE}
num_flights_going_from_Aus = abia_austin_Dep %>%
  group_by(Dest)  %>%  
  summarize(DestinationCity = length(Dest))

num_flights_going_To_Aus = abia_austin_Arr %>%
  group_by(Origin)  %>%  
  summarize(OriginCity = length(Origin))

num_flights_going_from_Aus=as.data.frame(num_flights_going_from_Aus)
num_flights_going_To_Aus = as.data.frame(num_flights_going_To_Aus)

top_ten_dest=num_flights_going_from_Aus[order(-num_flights_going_from_Aus$DestinationCity),][1:10,]
top_ten_ori =  num_flights_going_To_Aus[order(-num_flights_going_To_Aus$OriginCity),][1:10,]
colnames(top_ten_dest)[1] <- "City"
colnames(top_ten_ori)[1] <- "City"
top_ten_city = merge(top_ten_dest, top_ten_ori, by="City")
top_ten_city <- melt(top_ten_city, id.vars = 'City')
```

The plot below analyzes the top ten airports to which passengers fly to and fly in from.
Dallas and Houston are by far the most popular destinations.
```{r echo=FALSE, fig.align = "center",fig.width = 6,fig.height = 4}
ggplot(top_ten_city, aes(x=City, y=value, fill=variable )) + 
  geom_bar(stat='identity', position='dodge') +
  labs(title="Top 10 cities for incoming and outgoing flights", 
       y="Number of Flights",
       x = "City",
       fill="Type")+
  theme(plot.title = element_text(hjust = 0.5))
```

**Average time spent flying by Airline**\
```{r include=FALSE}
Avg_Time_By_Carrier = abia_austin_Dep %>%
  group_by(UniqueCarrier)  %>%  
  summarize(Avg_Time = mean(ActualElapsedTime, na.rm = TRUE))

```

The plot below analyzes the average time spent flying for each airline.  JetBlue (B6) flies for more than 3 hours on average.
```{r echo=FALSE, fig.align = "center",fig.width = 6,fig.height = 4}
ggplot(Avg_Time_By_Carrier, aes(x=UniqueCarrier, y=Avg_Time)) + 
  geom_bar(stat='identity') +
  labs(title="Average Air Time for each airline", 
       y="Avg Time",
       x = "Airline Carrier Code")+
  theme(plot.title = element_text(hjust = 0.5))
```

**Actual minus Scheduled Departure Times**\
```{r include=FALSE}
Avg_Time_By_Carrier = abia_austin_Dep %>%
  group_by(UniqueCarrier)  %>%  
  summarize(Avg_Time = mean(ActualElapsedTime, na.rm = TRUE))
abia_austin_Dep=transform(abia_austin_Dep, Diff_Category=ifelse(Diff_DepTime>=0 & Diff_DepTime<10, "0-10", 
                                         ifelse(Diff_DepTime>=10 & Diff_DepTime<20, "10-20",
                                                ifelse(Diff_DepTime>=20 & Diff_DepTime<30, "20-30",
                                                       ifelse(Diff_DepTime>=30, "30+",
                                                              ifelse(Diff_DepTime<0 & Diff_DepTime>=-10, "-10-0",
                                                                     ifelse(Diff_DepTime<-10, "-10+")))))))

Diff_Dep_Time = abia_austin_Dep %>%
  group_by(Diff_Category)  %>%  
  summarize(Times = length(Diff_Category))
Diff_Dep_Time=subset(Diff_Dep_Time, !is.na(Diff_Category))
temp=Diff_Dep_Time
temp[1,]=Diff_Dep_Time[2,]
temp[2,]=Diff_Dep_Time[1,]
Diff_Dep_Time=temp
```

The plot below analyzes on average how often an airline deviates from its scheduled departure time.  Most airlines leave between 0 to 10
minutes earlier than scheduled!
```{r echo=FALSE, fig.align = "center",fig.width = 6,fig.height = 4}
ggplot(Diff_Dep_Time, aes(x=reorder(Diff_Category, -Times), y=Times)) + 
  geom_bar(stat='identity')+
  labs(title="Actual minus Scheduled Departure Time", 
       y="Number of Flights",
       x = "Time Difference (Minutes)")+
  theme(plot.title = element_text(hjust = 0.5))
```

# **Problem 3: Portfolio modeling**

**Background**\
For this problem, we are analyzing five different ETFs ranging from Gold ETFs
to Oil related ETFs.  We chose these ETFs because we wanted to explore a diverse set of ETFs based on volatility.\

We have chosen to go with 5 ETFs: \
“GLD” - The Fund seeks to achieve the performance of gold bullion less the expenses of the Fund\

“USO” - The Fund seeks to reflect the performance of the spot price of West Texas Intermediate light, sweet crude oil delivered to Cushing, Oklahoma by investing in a mix of Oil Futures Contracts and Other Oil Interests.\

“VNQ” - The Fund seeks to provide a high level of income and moderate long-term capital appreciation by tracking the performance of a benchmark index that measures the performance of publicly traded equity REITs and other real estate-related investments.\

“BNO” - BNO tracks the Brent oil spot price using near-month ICE futures contracts.\

“SLV” - The Fund seeks to reflect generally the performance of the price of silver. \

```{r include=FALSE}
mystocks <- c("GLD","USO","VNQ","BNO","SLV")
getSymbols(mystocks,from = "2014-01-01")

###

GLDa <- adjustOHLC(GLD)
USOa <- adjustOHLC(USO)
VNQa <- adjustOHLC(VNQ)
BNOa <- adjustOHLC(BNO)
SLVa <- adjustOHLC(SLV)
```

**Volatility**\
Below are a few plots for the closing prices of ETF. The oil ETFs are the most volatile of the five funds chosen.
```{r ,echo=FALSE,fig.align = "center",fig.width = 5,fig.height = 3}
plot(ClCl(GLDa),main = "GLD Price since 2014",
     xlab = "Amount in Dollars")
plot(ClCl(USOa),main = "USO Price since 2014",
     xlab = "Amount in Dollars")
plot(ClCl(VNQa),main = "VNQ Price since 2014",
     xlab = "Amount in Dollars")
plot(ClCl(BNOa),main = "BNO Price since 2014",
     xlab = "Amount in Dollars")
plot(ClCl(SLVa),main = "SLV Price since 2014",
     xlab = "Amount in Dollars")
```

```{r include=FALSE}
returns <- cbind(ClCl(GLDa),ClCl(USOa),ClCl(VNQa),ClCl(BNOa),ClCl(SLVa))
all.r <- as.matrix(na.omit(returns))
n <- nrow(all.r)
```

**Portfolios**\
**Portfolio 1** : A portfolio of equal weights to all ETFs (i.e, 20 percent to all ETFs)
```{r echo=FALSE,message=FALSE,fig.align = "center",fig.width = 5,fig.height = 3}
boot.portfolio1 <- c()
set.seed(1)
for (i in 1:5000){
  total <- 100000
  weights <- rep(0.2,5)
  holdings <- total * weights
  for (i in 1:20){
    return.day <- resample(all.r,1,orig.ids = FALSE)
    holdings <- holdings*(1+return.day)
    #wealth <- c(wealth,holdings)
  }
  boot.portfolio1 <- c(boot.portfolio1,sum(holdings))
}

return1 <- data.frame(boot.portfolio1-100000)
colnames(return1) <- "Returns"
ggplot(return1, aes(x = Returns), 
       ylab = "Freq") +
      labs(title = "Portfolio 1") + 
      geom_histogram(color = "black",fill = "white") + 
      theme(plot.title = element_text(hjust = 0.5))
```

```{r echo=FALSE}
print(abs(quantile(return1$Returns,prob = 0.05)))
print(mean(return1$Returns))
```
The 5% value at risk for this particular portfolio is roughly $9,674.  The average return on the portfolio is a loss of \$392.\

**Portfolio 2** : A portfolio that invests 96 percent of wealth into gold and 1 percent into each of the remaining 4 ETFs.
```{r echo=FALSE,message=FALSE,fig.align = "center",fig.width = 5,fig.height = 3}
boot.portfolio2 <- c()
set.seed(1)
for (i in 1:5000){
  total <- 100000
  weights <- c(0.96,0.01,0.01,0.01,0.01)
  holdings <- total * weights
  for (i in 1:20){
    return.day <- resample(all.r,1,orig.ids = FALSE)
    holdings <- holdings*(1+return.day)
    #wealth <- c(wealth,holdings)
  }
  boot.portfolio2 <- c(boot.portfolio2,sum(holdings))
}

return2 <- data.frame(boot.portfolio2-100000)
colnames(return2) <- "Returns"
ggplot(return2, aes(x = Returns), 
       ylab = "Freq") +
      labs(title = "Portfolio 2") + 
      geom_histogram(color = "black",fill = "white") + 
      theme(plot.title = element_text(hjust = 0.5))
```

```{r echo=FALSE}
print(abs(quantile(return2$Returns,prob = 0.05)))
print(mean(return2$Returns))
```
The 5% value at risk for this particular portolio is roughly \$5,670.  The average return on the portfolio is a profit of \$491.\


**Portfolio 3** : A portfolio that invests 60 percent of wealth into VNQ and 10 percent into each of the remaining 4 ETFs.

```{r, echo=FALSE,message=FALSE,fig.align = "center",fig.width = 5,fig.height = 3}
boot.portfolio3 <- c()
set.seed(1)
for (i in 1:5000){
  total <- 100000
  weights <- c(0.1,0.1,0.6,0.1,0.1)
  holdings <- total * weights
  for (i in 1:20){
    return.day <- resample(all.r,1,orig.ids = FALSE)
    holdings <- holdings*(1+return.day)
    #wealth <- c(wealth,holdings)
  }
  boot.portfolio3 <- c(boot.portfolio3,sum(holdings))
}

return3 <- data.frame(boot.portfolio3-100000)
colnames(return3) <- "Returns"
ggplot(return3, aes(x = Returns), 
       ylab = "Freq") +
      labs(title = "Portfolio 3") + 
      geom_histogram(color = "black",fill = "white") + 
      theme(plot.title = element_text(hjust = 0.5))

```

```{r echo=FALSE}
print(abs(quantile(return3$Returns,prob = 0.05)))
print(mean(return3$Returns))
```
The 5% value at risk of this particular portfolio is roughly \$7,695.  The average return on this portfolio is a profit of \$226.\

**Report**\
Based on our analysis, portfolio 2 performed the best. By investing 96% of our wealth into the gold ETF, we were able to achieve the highest returns and the lowest VaR (value at risk) at 5% between all portfolios. This is an interesting result as diversification of the 
portfolio hurt our investments which suggests that ETFs related to Oil and Silver 
are significantly more volatile than Gold. This also suggests that Gold is typically a safe investment to make.\

**Problem 4: Market Segmentation**\
From the dataset provided by the company "NutrientH20", we hope to extract some vital market information regarding the types of followers that "NutrientH20" has.  
```{r include = F}
social <- read.csv("social_marketing.csv",header = T)
scaled <- as.data.frame(scale(social[,-c(1)],center = T,scale = T))
ID <- as.data.frame(social[,1])
```

We first perform dimension reduction on the dataset to improve computational ability.  In addition, we can visualize the marginal variance explained by adding another PC.  Because the elbow is not clear in this plot, we choose a value of 15 PCs to continue our analysis.
```{r,echo = F,fig.align = "center",fig.width = 6,fig.height = 4}
pca.social <- prcomp(scaled)
qplot(seq_along(summary(pca.social)$importance[3,]),
      summary(pca.social)$importance[3,], 
      xlab = "PC",
      ylab = "Cumulative Variance Explained")
```


```{r echo = F}
scaled.pca <- as.data.frame(pca.social$x[,1:15])
loadings <- rownames_to_column(as.data.frame(pca.social$rotation),"Category")
```

Based on our understanding of the dataset, we can conclude which PCs are associated with separating which types of followers.  For example, in the plot below, we can see that the third PC has weights that are strongly positive for people that are interested in fitness, but not so much in computers/gaming/politics.
```{r ,echo = FALSE,fig.align = "center",fig.width = 6,fig.height = 4}
fit <- select(loadings,Category,PC3)
sorted <- arrange(fit,desc(PC3))
ggplot(sorted, aes(x=reorder(Category,PC3), y=PC3)) +
  geom_bar(stat='identity') + coord_flip() +
  ylab("Category")
```
In contrast, our fourth PC (below) has strong negative weights for health and fitness and seems to value online gaming and sports.
```{r, echo = FALSE, fig.align = "center",fig.width = 6,fig.height = 4}
gamer <- select(loadings,Category,PC4)
sorted <- arrange(gamer,desc(PC4))
ggplot(sorted, aes(x=reorder(Category,PC4), y=PC4)) +
  geom_bar(stat='identity') + coord_flip() + 
  ylab("Category")
```
These two PCs, then would be good at separating and visualizing different types of followers in a 2-Dimensional space.
```{r, echo = FALSE,fig.align = "center",fig.width = 6,fig.height = 4}
scaled.pca <- as.data.frame(pca.social$x[,1:15])
scaled.points <- select(scaled.pca,PC3,PC4)
ggplot(scaled.points,aes(PC3,PC4)) + 
  geom_point()
```
In order to determine clusters of followers in this space, we perform kmeans clustering on our points in PC space. To determine the optimal hyperparameter for clustering, we try several values of *k* and measure the best one using our within sum of squares as the metric.  
```{r,echo = FALSE, warning=F}
scaled <- as.data.frame(scale(social[,-c(1)],center = T,scale = T))
mu <- attr(scale(social[,-c(1)]), "scaled:center")
sigma <- attr(scale(social[,-c(1)]),"scaled:scale")
```

```{r,echo = FALSE,fig.align = "center", warning=FALSE}
withinss = c()
seed <- c()
for(k in 2:20){
  set.seed(1)
  cluster_k = kmeans(scaled, k, nstart=25)
  withinss= c(withinss, cluster_k$tot.withinss)
}
qplot(seq_along(withinss),withinss,xlab = "k")
```
Based on the plot, we decided to choose $k=7$.  We can now visualize the different clusters in the 2D PC space we chose earlier.  There seems to be a clear separation between clusters - those with a higher positive value for PC3 but low positive value for PC4 are individuals who love fitness (cluster 5).  Those who (we suspect) are college students are going to be in the positive PC4 range with negative PC3 values (cluster 4).
```{r,echo = FALSE,fig.align = "center",fig.width = 6,fig.height = 4}
set.seed(1)
cluster <- kmeans(scaled, 7, nstart=25)
scaled.points$cols <- cluster$cluster

qplot(PC3,PC4,data = scaled.points,color = factor(cols)) + 
  geom_point()
```
A closer examination of the clusters as determined by kmeans gives us a better picture for what type of followers are within this group.
```{r, echo = FALSE,fig.align = "center",fig.width = 6,fig.height = 4}
clust3 <- as.data.frame(sort((cluster$center[3,]*sigma + mu), decreasing=TRUE))
clust3$category <- rownames(clust3)
clust5 <- as.data.frame(sort((cluster$center[5,]*sigma + mu), decreasing=TRUE))
clust5$category <- rownames(clust5)
kmeansDF <- merge(clust3,clust5, by = "category")
colnames(kmeansDF)<- c("category","Cluster_3","Cluster_5")
ggplot(kmeansDF, aes(x=reorder(category,Cluster_3), y=Cluster_3)) +
  geom_bar(stat='identity') + coord_flip() + 
  ylab("Category") + xlab("Cluster 3")
ggplot(kmeansDF, aes(x=reorder(category,Cluster_5), y=Cluster_5)) +
  geom_bar(stat='identity') + coord_flip() +
  ylab("Category") + xlab("Cluster 5")
```


**Problem 5: Author Attribution**\

Using the ReutersC50 dataset, we hoped to fit models that could predict the author of a particular article based on the article's textual content alone.  \

**Data Pre-Processing**\

We first needed to ensure that the test and training datasets had the appropriate format. The datasets need to be formatted into count matrices, where each $i,j$ entry represents the number of $j$ words that the $i^{th}$ document contains (using DocumentTermMatrix function from the "tm" package in R). \


Using several pre-processing techniques found in the "tm" package, we convert each word within each article into lower case.  We also strip each word of numbers, punctuation, and whitespaces, before then creating the data matrices.\

Since both the training and testing data matrices are sparse, we filter out terms that have more than 98% sparse terms (0's).  We choose this filter as it preserves words that specific authors  use, which would benefit our predictions.  We use the removeSparseTerms function from the "tm" package in R.\

Given the filtered dataset, we then convert each $i,j$ entry into a TF-IDF score using the weightTfIdf function in the "tm" package.\

In addition, we keep track of which document belongs to which author by creating an additional vector (for both the test and train datasets) where the $i^{th}$ element of the list is the author of document $i$.\
```{r ,message = FALSE,include = FALSE, warning=FALSE}
# setwd("C:/Users/timot/Documents/GitHub/PMAssignment/")
library(tidyverse)
library(tm)
library(slam)
library(proxy)
readerPlain = function(fname){
  readPlain(elem=list(content=readLines(fname)), 
            id=fname, language='en') }

file_list <- Sys.glob("ReutersC50/C50train/*")

total <- c()
author <- c()

for (i in 1:length(file_list)){
  articles <- Sys.glob(as.character(paste(file_list[i],"/*.txt", sep = "")))
  author <- c(author,rep(strsplit(file_list[i],"/")[[1]][3],length(articles)))
  total <- c(total,articles)
}

data <- cbind(author,total)
data <- cbind(data,lapply(data[,2],readerPlain))


documents_raw <- Corpus(VectorSource(data[,3]))

my_documents <- documents_raw %>%
  tm_map(content_transformer(tolower))  %>%             
  tm_map(content_transformer(removeNumbers)) %>%        
  tm_map(content_transformer(removePunctuation)) %>%    
  tm_map(content_transformer(stripWhitespace))          

my_documents <- tm_map(my_documents, content_transformer(removeWords), stopwords("en"))

DTM_train <- DocumentTermMatrix(my_documents)
tm::inspect(DTM_train[1:5,1:5])
DTM_train <- removeSparseTerms(DTM_train, 0.95)
tfidf_train = weightTfIdf(DTM_train)
```
Below is a sample of our training dataset:
```{r echo = FALSE}
tm::inspect(tfidf_train[1:5,1:5])
```

```{r echo = FALSE, warning=FALSE}
file_list2 <- Sys.glob("ReutersC50/C50test/*")

total2 <- c()
author2 <- c()

for (i in 1:length(file_list2)){
  articles2 <- Sys.glob(as.character(paste(file_list2[i],"/*.txt", sep = "")))
  author2 <- c(author2,rep(strsplit(file_list2[i],"/")[[1]][3],length(articles2)))
  total2 <- c(total2,articles2)
}

data2 <- cbind(author2,total2)
data2 <- cbind(data2,lapply(data2[,2],readerPlain))


documents_raw2 <- Corpus(VectorSource(data2[,3]))

my_documents2 <- documents_raw2 %>%
  tm_map(content_transformer(tolower))  %>%             
  tm_map(content_transformer(removeNumbers)) %>%        
  tm_map(content_transformer(removePunctuation)) %>%    
  tm_map(content_transformer(stripWhitespace))          

my_documents2 <- tm_map(my_documents2, content_transformer(removeWords), stopwords("en"))

DTM_test <- DocumentTermMatrix(my_documents2)
tm::inspect(DTM_test[1:5,1:5])

DTM_test <- removeSparseTerms(DTM_test, 0.95)
tfidf_test <- weightTfIdf(DTM_test)
```
We then filter words that are uninformative and appear in every document (i.e. all entries are 0).
```{r echo = FALSE}
X_train <- as.matrix(tfidf_train)
scrub <- which(colSums(X_train) == 0)
X_train <- X_train[,-scrub]

X_test <- as.matrix(tfidf_test)
scrub2 <- which(colSums(X_test) == 0)
X_test <- X_test[,-scrub2]

train.cols <- colnames(X_train)
test.cols <- colnames(X_test)
```
Due to our filtering preference, we found that the words (features) between the training dataset and testing dataset matched up perfectly.  Because of this, no other steps were needed to fix the features between the datasets.\
```{r echo = F}
match <- intersect(train.cols,test.cols)
X_test <- X_test[,match]
X_train <- X_train[,match]
```

**Dimension Reduction**\

We then perform PCA on our training dataset.  To ensure the principal components between both the training and testing dataset are equivalent, we project the observations of the test set into the PC space as defined by our training dataset.  We choose the first 500 PCs to improve computational speed (~85% of variance explained).
```{r echo = FALSE,fig.align = "center",fig.width = 6,fig.height = 4}
pca.x <- prcomp(X_train,scale=T)
qplot(seq_along(summary(pca.x)$importance[3,]),summary(pca.x)$importance[3,],xlab = "PC",ylab = "Cumulative Variance Explained")

train <- pca.x$x[,1:500]
test <- predict(pca.x,newdata = X_test)[,1:500]
```

**Prediction**\

We use the random forest and k-Nearest Neighbor algorithms to determine a model for prediction.\

We first try the random forest algorithm on our original dataset, rather than PC space.  We have 725 words, and choose 70 as the number of variables to consider for each split as well as 500 trees for computational speed. We were able to achieve a classification accuracy of approximately 58%.  *Note: we drop the feature (word) "next" from the dataset, as the random forest function throws an error when it encounters this word.*
```{r echo = FALSE}
newtrain <- as.data.frame(cbind(author,X_train))[,-170]
newtest <- as.data.frame(cbind(author2,X_test))[,-170]

library(randomForest)

set.seed(1)
rf <- randomForest(factor(author)~.,data = newtrain,importance = T,mtry = 70,ntree = 500)
preds <- predict(rf,newdata = newtest)
accuracy <- mean(preds==as.factor(author2))
accuracy
```


In random forest, we chose several hyperparameters that could balance between bias and variance, while trying to account for computational speed.  We designated a subset of 20 variables for each split, and we grew 500 trees.
```{r echo = FALSE}
train <- as.data.frame(cbind(author,train))
test <- as.data.frame(cbind(author2,test))
set.seed(1)
rf <- randomForest(as.factor(author)~.,data = train,importance = T,mtry = 70, ntree =500)

preds <- predict(rf,newdata = test)
accuracy <- mean(preds==as.factor(author))
accuracy
```
Using the random forest algorithm on our PC space, we were able to achieve a roughly 44% accuracy on our dataset.  While it does reduce our dimensionality, it also reduces quite a bit of accuracy.\

We then fit a k-Nearest Neighbors model on our training dataset, and chose to use $k = 6$.

```{r echo = FALSE,fig.align = "center",fig.width = 6,fig.height = 4}
library(class)
set.seed(1)
accuracy <- c()
for (i in seq(1,50, by = 5)){
  knn <- knn(train[,-c(1)],test[,-c(1)],cl = factor(train$author), k = i)
  accuracy <- c(accuracy,mean(knn == factor(test$author2)))
}

qplot(seq(1,50, by = 5),accuracy,xlab = "k",geom = c("point","line"))
```
In this situation, $k=6$ provided the best accuracy.\

In general, our random forest model fit on the original dataset (not PC space) performed the best.  

**Problem 6: Association Rule Mining**\
In this problem, we wanted to find association rules within a dataset containing baskets of grocery shoppers.  We first take the necessary data pre-processing steps to read in the data, as well as format it in a way that allows the "arules" package in R to work.  Below is a summary of the resulting object from the class "transactions" (also within the "arules" package).\

In our dataset, we can see that whole milk is purchased in roughly 25% of the baskets.\
```{r, include = FALSE}
library(arules)
library(arulesViz)
grocery <- read.delim("groceries.txt",sep = "\n",header = F)

baskets <- c()
for (i in 1:length(rownames(grocery))){
  current <- strsplit(grocery[i,],split = ",")
  baskets <- c(baskets,current)
}
```


```{r, echo = FALSE}
baskets.trans <- as(baskets,"transactions")
summary(baskets.trans)
```

We tested several thresholds for both confidence and lift.  In general, what we were looking for were rules that had a high number of counts (in terms of the left-hand side of the rule) and high confidence as well as lift.  \

Below is a small sample of our baseline list of rules.  We choose a confidence of 0.1 and a lift of 1.  We choose a max length of 3 in order to find more general rules, instead of finding rules with larger lengths that may not occur as often.  In total, there are 18597 rules.  
```{r include = FALSE,fig.align = "center",fig.width = 6,fig.height = 4}
basket.rules <- apriori(baskets.trans, parameter=list(support=.001, confidence=.1, maxlen=3))
sub <- subset(basket.rules, subset=lift > 1)
first20=arules::inspect(sub)[1:20,]
```

```{r echo=FALSE, fig.align = "center",fig.width = 6,fig.height = 4}
arules::inspect(sub[1:20])
plot(head(basket.rules, 20, by='lift'), method='graph')
```


In addition, from the graph, we can see by increasing the confidence (as well as reducing the minimum support required), we can see a drastic decrease in the number of rules in our list.  These rules are generally good, because they have high count and have a good level of confidence.
```{r echo = FALSE,fig.align = "center",fig.width = 6,fig.height = 4}
basket.rules2 <- apriori(baskets.trans, parameter=list(support=.05, confidence=.3, maxlen=3))
sub <- subset(basket.rules2, subset=lift > 1)
arules::inspect(sub)
plot(head(sub, 20, by='lift'), method='graph')
```

By decreasing the required minimum support again, we can achieve a slightly larger list (size 37) of rules.  However, in general, the counts for each rule is low (around 200 each rule).\

Using the same rules, we can filter out the items that have a high value for lift.  In general, since lift is high, it implies that the items on either side of the rule are not independent from one another (i.e. they are complementary).  An interesting rule to note on this list is the second rule - the count is high and the confidence is good.  It also makes sense - typically, when a shopper is buying vegetables, they probably won't just stop at root vegetables.  
```{r echo = FALSE,fig.align = "center",fig.width = 6,fig.height = 4}
basket.rules3 <- apriori(baskets.trans, parameter=list(support=.02, confidence=.3, maxlen=3))
sub <- subset(basket.rules3, subset=lift > 2)
arules::inspect(sub)
plot(head(sub, 20, by='lift'), method='graph')

```

We then reduced the minimum required support even further and increased the maximum length of each rule.  In general, most of these rules are good (if we disregard the low count).  They all have lifts greater than 2, which is indicative of their possible non-independence. 
```{r echo = FALSE,fig.align = "center",fig.width = 6,fig.height = 4}
basket.rules4 <- apriori(baskets.trans, parameter=list(support=.01, confidence=.3, maxlen=4))
sub <- subset(basket.rules4, subset=lift > 2)
arules::inspect(sub)
plot(head(sub, 20, by='lift'), method='graph')
```

We then looked for substitute items.  After reducing support and maximum length slightly, we found an interesting set of rules that depicted this.  In general, we believe that it's safe to assume that most people who go shopping are not there to buy shopping bags, but rather are there to buy whole milk.\
```{r echo = FALSE}
basket.rules5 <- apriori(baskets.trans, parameter=list(support=.005, confidence=.01, maxlen=3))
arules::inspect(subset(basket.rules5, subset=lift < 1)[76:77])
```

In general, most of the rules make sense.  Root vegetables aren't going to be the only vegetables you buy, you'll probably buy other vegetables.  If you're getting yogurt, you're going to be in the dairy section, so you'll probably pick up some milk too.  And finally, if you're going to make a sandwich, you'll probably need both white bread and ham. \



