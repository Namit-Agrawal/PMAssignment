---
title: "Exercise"
author: "Namit Agrawal, Timothy Cheng"
date: "08/16/20"
output:
  pdf_document: default
---

## **Problem 1: Visual story telling: green buildings**
```{r include=FALSE}
library(mosaic)
library(ggplot2)
library(reshape)
library(chron)
library(quantmod)
library(tidyverse)
```

```{r include=FALSE}
greenBuild = read.csv("greenbuildings.csv", stringsAsFactors = FALSE)
greenBuild$green_rating = as.factor(greenBuild$green_rating)
greenBuild$class_a = as.factor(greenBuild$class_a)
greenBuild$renovated = as.factor(greenBuild$renovated)
greenBuild$amenities = as.factor(greenBuild$amenities)

attach(greenBuild)
```

It does make sense to use the median rather than mean as the non-green buildings
have many outliers as suggested by the boxplot below
```{r echo=FALSE, fig.align = "center",fig.width = 6,fig.height = 4}
ggplot(greenBuild, aes(x=green_rating, y=Rent)) + 
  geom_boxplot()
```


The stats guru is right about removing buildings with less than 10%  occupancy
as based on the summary below. Within the group of buildings that have only 10% occupancy, there is only one building with a green rating.  In addition, roughly half of the buildings have 3 stories and very few buildings have a Class A designation.  Hence
we should remove buildings with less than 10% occupancy as it may distort the analysis.
```{r echo=FALSE}
summary(subset(greenBuild, leasing_rate<10))
```

The median rent for green buildings and non-green buildings is correct if buildings
with more than 10% occupancy rate are considered.
```{r echo=FALSE}
greenBuild90 = subset(greenBuild, leasing_rate>=10)
medianRent = greenBuild90 %>% 
  group_by(green_rating)  %>%  
  summarize(MedianRent = median(Rent, na.rm = TRUE)) 
medianRent
```
Assuming that the building is 250000 square feet, it seems that the stats guru is correct about
recuperating the costs in a little under 8 years.

**Confounding variables investigation**\

**Renovated Buildings**\
Based on the numerical summary analysis for buildings that are renovated,
it does not seem there is much confounding going on as the median rents 
for non renovated and renovated buildings are similar especially for green buildings

```{r echo=FALSE}
renovated_group = greenBuild90 %>%
  group_by(renovated, green_rating)  %>%  
  summarize(MedianRent = median(Rent, na.rm = TRUE)) 
renovated_group
```

```{r echo=FALSE}
renovated_group_rent = greenBuild90 %>%
  group_by(renovated)  %>%  
  summarize(MedianRent = median(Rent, na.rm = TRUE)) 
renovated_group_rent
```

**Number of Stories**\
As suggested by the plot below, the median for stories is a valid selection since there are some outliers.
It looks there is not much evidence of confounding for the number of stories in the building, even though there is a slight increase in rent as the number of stories goes up. The median for stories of green buildings only differs by 1, so stories may not directly be affecting the rent.

```{r echo=FALSE,fig.align = "center",fig.width = 6,fig.height = 4}
ggplot(data=greenBuild90) + 
  geom_point(mapping=aes(x=stories, y=Rent, colour=green_rating)) +
  labs(x="Stories", y='Rent', title = 'Green buildings: Stories VS Rent',
       color='Green building')
```

```{r echo=FALSE}
stories_group = greenBuild90 %>%
  group_by(green_rating)  %>%  
  summarize(MedianStories = median(stories, na.rm = TRUE)) 
stories_group
```

**Age**\

An initial analysis provides a stark contrast in age between green and non-green buildings.
```{r,echo = FALSE,fig.align = "center",fig.width = 6,fig.height = 4}
ggplot(greenBuild, aes(x=green_rating, y=age)) + 
  geom_boxplot()
```
However, it looks like there is no confounding for age, as there is no correlation between
the age of the building and the rent from the plot below.
```{r echo=FALSE,fig.align = "center",fig.width = 6,fig.height = 4}
ggplot(data=greenBuild90) + 
  geom_point(mapping=aes(x=age, y=Rent, colour=green_rating)) +
  labs(x="Age", y='Rent', title = 'Green buildings: Age VS Rent',
       color='Green building')
```

```{r echo=FALSE}
age_group = greenBuild90 %>%
  group_by(green_rating)  %>%  
  summarize(MedianAge = median(age, na.rm = TRUE)) 
age_group
```

**Size**\
It looks like size is definitely a confounding variable, as size is correlated 
with rent from plot below and the median size for green buildings is double that of non-green. Thus, there is a premium in rent for larger sizes, as expected.
```{r echo=FALSE}
size_group = greenBuild90 %>%
  group_by(green_rating)  %>%  
  summarize(MedianSize = median(size, na.rm = TRUE)) 
size_group
```

```{r echo=FALSE,fig.align = "center",fig.width = 6,fig.height = 4}
ggplot(data=greenBuild90) + 
  geom_point(mapping=aes(x=size, y=Rent, colour=green_rating)) +
  labs(x="Size", y='Rent', title = 'Green buildings: size VS Rent',
       color='Green building')
```

**Cluster Rent**\
There does not seem to be confounding for cluster rent, as the median for cluster 
rent is approximately the same between green and non-green buildings. However, cluster rent is highly
correlated with the rent of the building.
```{r echo=FALSE}
cluster_rent_group = greenBuild90 %>%
  group_by(green_rating)  %>%  
  summarize(MedianClusterRent = median(cluster_rent, na.rm = TRUE)) 
cluster_rent_group
```

```{r echo=FALSE,fig.align = "center",fig.width = 6,fig.height = 4}
ggplot(data=greenBuild90) + 
  geom_point(mapping=aes(x=cluster_rent, y=Rent, colour=green_rating)) +
  labs(x="Cluster Rent", y='Rent', title = 'Green buildings: Cluster rent VS Rent',
       color='Green building')
```

**Class**\
It looks like the Class designation of buildings is a confounding variable, as Class A buildings
have generally higher median rents - in addition, having a green_rating with the Class A designation drives median rents even higher. Class A designated buildings seem to correlate with overall rent as
well.
```{r echo=FALSE}
classa_group= greenBuild90 %>%
  group_by(class_a, green_rating)  %>%  
  summarize(MedianRent = median(Rent, na.rm = TRUE)) 
classa_group
```

```{r,echo = FALSE,fig.align = "center",fig.width = 6,fig.height = 4}
ggplot(greenBuild, aes(x=class_a, y=Rent)) + 
  geom_boxplot()
```

```{r echo=FALSE}
classa_group_rent= greenBuild90 %>%
  group_by(class_a)  %>%  
  summarize(MedianRent = median(Rent, na.rm = TRUE)) 
classa_group_rent
```

**Thoughts**\
From the investigation above it seems like size and class are the only
confounding variables, as the green buildings tend to have larger spaces
and larger spaces have higher rent. However, the stats guru is only taking into account
the median rent of all the building with more than 10% occupancy.  If we apply another filter to include only 15 story buildings, we see that the rent goes up 
drastically for green buildings - all the way to 37 dollars. However, it may not
be wise to use this filter as there is only 10 green buildings that have 15 stories.
```{r echo=FALSE}
greenBuilding9015= subset(greenBuild90, stories==15)
stories_15_group = greenBuilding9015 %>%
  group_by(green_rating)  %>%  
  summarize(MedianRent = median(Rent, na.rm = TRUE), num=length(Rent)) 
stories_15_group
```

In general, the guru is correct with his analysis, but the analysis is performed on a dataset with a large range of different building specifications.  For example, the dataset only contains 10 green buildings that have 15 stories.  A larger sample size that adheres to the developers desired specs would provide more valid results.
## **Problem 2: Visual story telling: flights at ABIA**

**Most active airlines**\
```{r include=FALSE}
abia = read.csv("abia.csv", stringsAsFactors = FALSE)
abia$Month = as.factor(abia$Month)
abia$DayOfWeek = as.factor(abia$DayOfWeek)


abia$DepHour=sapply(abia$DepTime, function(x) x%/%100)
abia$ArrHour=sapply(abia$ArrTime, function(x) x%/%100)

abia$Diff_DepTime=difftime(as.POSIXct((times=sub("(\\d+)(\\d{2})", "\\1:\\2", abia$DepTime)),
           format='%H:%M'),  as.POSIXct((times=sub("(\\d+)(\\d{2})", "\\1:\\2", abia$CRSDepTime)),
                                        format='%H:%M'), units="mins")
abia$Diff_DepTime=as.numeric(abia$Diff_DepTime)
abia_austin_Dep = subset(abia, Origin == 'AUS')
abia_austin_Arr = subset(abia, Dest == 'AUS')

attach(abia)
```
Here we analyze which airlines are most active throughout the year in terms of
the distance flown. As seen by the plot, it appears Southwest (WN) and American
Airlines(AA) are the most active in flying out of ABIA and flying to ABIA.
```{r echo=FALSE, fig.align = "center",fig.width = 6,fig.height = 4}
ggplot(abia, aes(x=Month, y=Distance)) + 
  geom_bar(stat='identity') +geom_bar(stat='identity') +
  facet_wrap(~ UniqueCarrier, nrow = 4)+
  scale_x_discrete(guide = guide_axis(check.overlap = TRUE))+
  labs(title="Distance traveled by different airlines each month", 
       y="Distance",
       x = "Month")+
  theme(plot.title = element_text(hjust = 0.5))
```

**Delays by Day of Week**\
```{r include=FALSE}
delay_dep_sum = abia_austin_Dep %>%
  group_by(DayOfWeek)  %>%  
  summarize(ToDepDelay = sum(DepDelay, na.rm = TRUE)) 

delay_arr_sum = abia_austin_Arr %>%
  group_by(DayOfWeek)  %>%  
  summarize(ToArrDelay = sum(ArrDelay, na.rm = TRUE)) 
delay_sum = merge(delay_dep_sum, delay_arr_sum, by="DayOfWeek")
delay_sum <- melt(delay_sum, id.vars = 'DayOfWeek')

```
In the plot below, we analyze the departure and arrival delay for each day of the week.
There are more arrival delays than departure delays and Friday is the worst day to travel to/from Austin.
```{r echo=FALSE, fig.align = "center",fig.width = 6,fig.height = 4}
ggplot(delay_sum, aes(x=DayOfWeek, y=value, fill=variable )) + 
  geom_bar(stat='identity', position='dodge') +
  labs(title="departure and arrival delay for each day of week", 
       y="Total Delay Time",
       x = "Day Of the Week (1 = Monday)",
       fill="Delay")+
  theme(plot.title = element_text(hjust = 0.5))
```

**Average Delays per Airline**\
```{r include=FALSE}
num_dep_delay_by_carrier = abia_austin_Dep %>%
  group_by(UniqueCarrier)  %>%  
  summarize(AvgDepDelay = mean(DepDelay, na.rm = TRUE)) 

num_arr_delay_by_carrier = abia_austin_Arr %>%
  group_by(UniqueCarrier)  %>%  
  summarize(AvgArrDelay = mean(ArrDelay, na.rm = TRUE)) 

num_delay_by_carrier = merge(num_dep_delay_by_carrier, num_arr_delay_by_carrier, by="UniqueCarrier")
num_delay_by_carrier = as.data.frame(num_delay_by_carrier)
num_delay_by_carrier <- melt(num_delay_by_carrier, id.vars = 'UniqueCarrier')
```
This plot analyzes the average departure and arrival delay for each airline.  It looks like Piedmont Airlines (US) arrives and departs early on average as the delay time
is negative.
```{r echo=FALSE, fig.align = "center",fig.width = 6,fig.height = 4}
ggplot(num_delay_by_carrier, aes(x=UniqueCarrier, y=value, fill=variable )) + 
  geom_bar(stat='identity', position='dodge') +
  labs(title="Average departure and arrival delay for each airline", 
       y="Average Delay Time",
       x = "Airline Carrier Code",
       fill="Delay")+
  theme(plot.title = element_text(hjust = 0.5))

```

**Most common Delay types by Airline**\
```{r include=FALSE}
num_cancelled_by_carrier = abia %>%
  group_by(UniqueCarrier, CancellationCode)  %>%  
  summarize(Cancelprob = sum(Cancelled, na.rm = TRUE))

num_cancelled_dummy = abia %>%
  group_by(UniqueCarrier)  %>%  
  summarize(Total = length(Cancelled))
Cancellation_probs=merge(num_cancelled_by_carrier, num_cancelled_dummy, by="UniqueCarrier")
Cancellation_probs$True_prob = Cancellation_probs$Cancelprob/Cancellation_probs$Total
Cancellation_probs = subset(Cancellation_probs, CancellationCode=='A' | CancellationCode=='B' | CancellationCode=='C' )
Cancellation_probs$CancellationCode = ifelse(Cancellation_probs$CancellationCode=='A', "Carrier", ifelse(Cancellation_probs$CancellationCode =="B", "Weather", "NAS"))
```

Here we analyze the proportions of delays by airlines via the type of delay.
It look like most airlines suffer from carrier and weather delays.
```{r echo=FALSE, fig.align = "center",fig.width = 6,fig.height = 4}
ggplot(Cancellation_probs, aes(x=UniqueCarrier, y=True_prob, fill=CancellationCode )) + 
  geom_bar(stat='identity', position='dodge')+
  labs(title="Proportion of delay for each airline by type of delay", 
       y="Proportion",
       x = "Airline Carrier Code",
       fill="Type of Delay")+
  theme(plot.title = element_text(hjust = 0.5))
```

**Flights per hour of the Day**\
```{r include=FALSE}
num_flights_dep_per_hour = abia_austin_Dep %>%
  group_by(DepHour)  %>%  
  summarize(TotalDep = length(DepHour))

num_flights_arr_per_hour = abia_austin_Arr %>%
  group_by(ArrHour)  %>%  
  summarize(TotalArr = length(ArrHour))

num_flights_arr_per_hour=subset(num_flights_arr_per_hour, !is.na(ArrHour))
num_flights_dep_per_hour=subset(num_flights_dep_per_hour, !is.na(DepHour))
colnames(num_flights_arr_per_hour)[1] <- "Hour"
colnames(num_flights_dep_per_hour)[1] <- "Hour"

num_flights_per_hours = merge(num_flights_arr_per_hour, num_flights_dep_per_hour, by="Hour")
num_flights_per_hours <- melt(num_flights_per_hours, id.vars = 'Hour')
```

The plot below analyzes what are most frequent departure and arrival times.  Passengers typically fly *out* early in the morning and fly *in* late at night. Between
noon and evening there is an even split between passengers flying in and out.
```{r echo=FALSE, fig.align = "center",fig.width = 7,fig.height = 4}
ggplot(num_flights_per_hours, aes(x=Hour, y=value, fill=variable )) + 
  geom_bar(stat='identity', position='dodge') +
  labs(title="Number of flights at ABIA for each hour by departure and arrival", 
       y="Number of Flights",
       x = "Hour",
       fill="Departure-Arrival")+
  theme(plot.title = element_text(hjust = 0.5))
```

**Flights by City**\
```{r include=FALSE}
num_flights_going_from_Aus = abia_austin_Dep %>%
  group_by(Dest)  %>%  
  summarize(DestinationCity = length(Dest))

num_flights_going_To_Aus = abia_austin_Arr %>%
  group_by(Origin)  %>%  
  summarize(OriginCity = length(Origin))

num_flights_going_from_Aus=as.data.frame(num_flights_going_from_Aus)
num_flights_going_To_Aus = as.data.frame(num_flights_going_To_Aus)

top_ten_dest=num_flights_going_from_Aus[order(-num_flights_going_from_Aus$DestinationCity),][1:10,]
top_ten_ori =  num_flights_going_To_Aus[order(-num_flights_going_To_Aus$OriginCity),][1:10,]
colnames(top_ten_dest)[1] <- "City"
colnames(top_ten_ori)[1] <- "City"
top_ten_city = merge(top_ten_dest, top_ten_ori, by="City")
top_ten_city <- melt(top_ten_city, id.vars = 'City')
```

The plot below analyzes the top ten airports to which passengers fly to and fly in from.
Dallas and Houston are by far the most popular destinations.
```{r echo=FALSE, fig.align = "center",fig.width = 6,fig.height = 4}
ggplot(top_ten_city, aes(x=City, y=value, fill=variable )) + 
  geom_bar(stat='identity', position='dodge') +
  labs(title="Top 10 cities for incoming and outgoing flights", 
       y="Number of Flights",
       x = "City",
       fill="Type")+
  theme(plot.title = element_text(hjust = 0.5))
```

**Average time spent flying by Airline**\
```{r include=FALSE}
Avg_Time_By_Carrier = abia_austin_Dep %>%
  group_by(UniqueCarrier)  %>%  
  summarize(Avg_Time = mean(ActualElapsedTime, na.rm = TRUE))

```

The plot below analyzes the average time spent flying for each airline.  JetBlue (B6) flies for more than 3 hours on average.
```{r echo=FALSE, fig.align = "center",fig.width = 6,fig.height = 4}
ggplot(Avg_Time_By_Carrier, aes(x=UniqueCarrier, y=Avg_Time)) + 
  geom_bar(stat='identity') +
  labs(title="Average Air Time for each airline", 
       y="Avg Time",
       x = "Airline Carrier Code")+
  theme(plot.title = element_text(hjust = 0.5))
```

**Average deviation off of Schedule Departure Time**\
```{r include=FALSE}
Avg_Time_By_Carrier = abia_austin_Dep %>%
  group_by(UniqueCarrier)  %>%  
  summarize(Avg_Time = mean(ActualElapsedTime, na.rm = TRUE))
abia_austin_Dep=transform(abia_austin_Dep, Diff_Category=ifelse(Diff_DepTime>=0 & Diff_DepTime<10, "0-10", 
                                         ifelse(Diff_DepTime>=10 & Diff_DepTime<20, "10-20",
                                                ifelse(Diff_DepTime>=20 & Diff_DepTime<30, "20-30",
                                                       ifelse(Diff_DepTime>=30, "30+",
                                                              ifelse(Diff_DepTime<0 & Diff_DepTime>=-10, "-10-0",
                                                                     ifelse(Diff_DepTime<-10, "-10+")))))))

Diff_Dep_Time = abia_austin_Dep %>%
  group_by(Diff_Category)  %>%  
  summarize(Times = length(Diff_Category))
Diff_Dep_Time=subset(Diff_Dep_Time, !is.na(Diff_Category))
temp=Diff_Dep_Time
temp[1,]=Diff_Dep_Time[2,]
temp[2,]=Diff_Dep_Time[1,]
Diff_Dep_Time=temp
```

The plot below analyzes on average how often an airline deviates from its scheduled departure time.  Most airlines leave between 0 to 10
minutes earlier than scheduled!
```{r echo=FALSE, fig.align = "center",fig.width = 6,fig.height = 4}
ggplot(Diff_Dep_Time, aes(x=reorder(Diff_Category, -Times), y=Times)) + 
  geom_bar(stat='identity')+
  labs(title="Actual minus Scheduled Departure Time", 
       y="Number of Flights",
       x = "Time Difference (Minutes)")+
  theme(plot.title = element_text(hjust = 0.5))
```

## **Problem 3: Portfolio modeling**

**Background**\
For this problem, we are analyzing five different ETFs ranging from Gold ETFs
to Oil related ETFs.\

We have chosen to go with 5 ETFs: \
“GLD” - The Fund seeks to achieve the performance of gold bullion less the expenses of the Fund\

“USO” - The Fund seeks to reflect the performance of the spot price of West Texas Intermediate light, sweet crude oil delivered to Cushing, Oklahoma by investing in a mix of Oil Futures Contracts and Other Oil Interests.\

“VNQ” - The Fund seeks to provide a high level of income and moderate long-term capital appreciation by tracking the performance of a benchmark index that measures the performance of publicly traded equity REITs and other real estate-related investments.\

“BNO” - BNO tracks the Brent oil spot price using near-month ICE futures contracts.\

“SLV” - The Fund seeks to reflect generally the performance of the price of silver. \

```{r include=FALSE}
mystocks <- c("GLD","USO","VNQ","BNO","SLV")
getSymbols(mystocks,from = "2014-01-01")

###

GLDa <- adjustOHLC(GLD)
USOa <- adjustOHLC(USO)
VNQa <- adjustOHLC(VNQ)
BNOa <- adjustOHLC(BNO)
SLVa <- adjustOHLC(SLV)
```

**Volatility**\
Below are a few plots for the closing prices of ETF. The oil ETFs are the most volatile of the five funds chosen.
```{r ,echo=FALSE,fig.align = "center",fig.width = 5,fig.height = 3}
plot(ClCl(GLDa),main = "GLD Price since 2014",
     xlab = "Amount in Dollars")
plot(ClCl(USOa),main = "USO Price since 2014",
     xlab = "Amount in Dollars")
plot(ClCl(VNQa),main = "VNQ Price since 2014",
     xlab = "Amount in Dollars")
plot(ClCl(BNOa),main = "BNO Price since 2014",
     xlab = "Amount in Dollars")
plot(ClCl(SLVa),main = "SLV Price since 2014",
     xlab = "Amount in Dollars")
```

```{r include=FALSE}
returns <- cbind(ClCl(GLDa),ClCl(USOa),ClCl(VNQa),ClCl(BNOa),ClCl(SLVa))
all.r <- as.matrix(na.omit(returns))
n <- nrow(all.r)
```

**Portfolios**\
**Portfolio 1** : A portfolio of equal weights to all ETFs (i.e, 20 percent to all ETFs)
```{r echo=FALSE}
boot.portfolio1 <- c()
set.seed(1)
for (i in 1:5000){
  total <- 100000
  weights <- rep(0.2,5)
  holdings <- total * weights
  for (i in 1:20){
    return.day <- resample(all.r,1,orig.ids = FALSE)
    holdings <- holdings*(1+return.day)
    #wealth <- c(wealth,holdings)
  }
  boot.portfolio1 <- c(boot.portfolio1,sum(holdings))
}

hist(boot.portfolio1,main = "Portfolio 1 Returns",
     xlab = "Amount in Dollars")
```

```{r echo=FALSE}
hist(boot.portfolio1 - 100000, breaks = 30,main = "Portfolio 1 profit/loss",
     xlab = "Amount in Dollars")
```

```{r echo=FALSE}
print(abs(quantile(boot.portfolio1 - 100000,prob = 0.05)))
print(mean(boot.portfolio1))
```
The 5% value at risk for this particular portfolio is roughly $9,674.\

**Portfolio 2** : A portfolio that invests 96 percent of wealth into gold and 1 percent into each of the remaining 4 ETFs.
```{r echo=FALSE}
boot.portfolio2 <- c()
set.seed(1)
for (i in 1:5000){
  total <- 100000
  weights <- c(0.96,0.01,0.01,0.01,0.01)
  holdings <- total * weights
  for (i in 1:20){
    return.day <- resample(all.r,1,orig.ids = FALSE)
    holdings <- holdings*(1+return.day)
    #wealth <- c(wealth,holdings)
  }
  boot.portfolio2 <- c(boot.portfolio2,sum(holdings))
}
```

```{r echo=FALSE}
hist(boot.portfolio2,main = "Portfolio 2 Returns",
     xlab = "Amount in Dollars")
```
```{r echo=FALSE}
hist(boot.portfolio2 - 100000, breaks = 30,main = "Portfolio 2 profit/loss",
     xlab = "Amount in Dollars")
```
```{r echo=FALSE}
print(abs(quantile(boot.portfolio2 - 100000,prob = 0.05)))
print(mean(boot.portfolio2))

```
The 5% value at risk for this particular portolio is roughly $5,670.\


**Portfolio 3** : A portfolio that invests 60 percent of wealth into VNQ and 10 percent into each of the remaining 4 ETFs.

```{r echo=FALSE}
boot.portfolio3 <- c()
set.seed(1)
for (i in 1:5000){
  total <- 100000
  weights <- c(0.1,0.1,0.6,0.1,0.1)
  holdings <- total * weights
  for (i in 1:20){
    return.day <- resample(all.r,1,orig.ids = FALSE)
    holdings <- holdings*(1+return.day)
    #wealth <- c(wealth,holdings)
  }
  boot.portfolio3 <- c(boot.portfolio3,sum(holdings))
}
```
```{r echo=FALSE}
hist(boot.portfolio3,main = "Portfolio 3 Returns",
     xlab = "Amount in Dollars")
```
```{r echo=FALSE}
hist(boot.portfolio3 - 100000, breaks = 30,main = "Portfolio 3 profit/loss",
     xlab = "Amount in Dollars")
```
```{r echo=FALSE}
print(abs(quantile(boot.portfolio3 - 100000,prob = 0.05)))
print(mean(boot.portfolio3))

```
The 5% value at risk of this particular portfolio is roughly $7,695.\

**Report**\
Based on our analysis, portfolio 2 performed the best. By investing 96% of our wealth into the gold ETF, we were able to achieve the highest returns and the lowest VaR (value at risk) at 5% between all portfolios. This is an interesting result as diversification of the 
portfolio hurt our investments which suggests that ETFs related to Oil and Silver 
are significantly more volatile than Gold. This also suggests that Gold is typically a safe investment to make.\

**Problem 4: Market Segmentation**\
From the dataset provided by the company "NutrientH20", we hope to extract some vital market information regarding the types of followers that "NutrientH20" has.  
```{r include = F}
social <- read.csv("social_marketing.csv",header = T)
scaled <- as.data.frame(scale(social[,-c(1)],center = T,scale = T))
ID <- as.data.frame(social[,1])
```

We first perform dimension reduction on the dataset to improve computational ability.  In addition, we can visualize the marginal variance explained by adding another PC.  Because the elbow is not clear in this plot, we choose a value of 15 PCs to continue our analysis.
```{r,echo = F,fig.align = "center",fig.width = 6,fig.height = 4}
pca.social <- prcomp(scaled)
qplot(seq_along(summary(pca.social)$importance[3,]),
      summary(pca.social)$importance[3,], 
      xlab = "PC",
      ylab = "Cumulative Variance Explained")
```


```{r echo = F}
scaled.pca <- as.data.frame(pca.social$x[,1:15])
loadings <- rownames_to_column(as.data.frame(pca.social$rotation),"Category")
```

Based on our understanding of the dataset, we can conclude which PCs are associated with separating which types of followers.  For example, in the plot below, we can see that the third PC has weights that are strongly positive for people that are interested in fitness, but not so much in computers/gaming/politics.
```{r ,echo = FALSE,fig.align = "center",fig.width = 6,fig.height = 4}
fit <- select(loadings,Category,PC3)
sorted <- arrange(fit,desc(PC3))
ggplot(sorted, aes(x=reorder(Category,PC3), y=PC3)) +
  geom_bar(stat='identity') + coord_flip()
```
In contrast, our fourth PC (below) has strong negative weights for health and fitness and seems to value online gaming and sports.
```{r}
gamer <- select(loadings,Category,PC4)
sorted <- arrange(gamer,desc(PC4))
ggplot(sorted, aes(x=reorder(Category,PC4), y=PC4)) +
  geom_bar(stat='identity') + coord_flip()
```
These two PCs, then would be good at separating and visualizing different types of followers in a 2-Dimensional space.
```{r}
scaled.pca <- as.data.frame(pca.social$x[,1:15])
scaled.points <- select(scaled.pca,PC3,PC4)
ggplot(scaled.points,aes(PC3,PC4)) + 
  geom_point()
```
In order to determine clusters of followers in this space, we perform kmeans clustering on our points in PC space. To determine the optimal hyperparameter for clustering, we try several values of *k* and measure the best one using our within sum of squares as the metric.  
```{r, warning=F}
scaled <- as.data.frame(scale(social[,-c(1)],center = T,scale = T))
mu <- attr(scale(social[,-c(1)]), "scaled:center")
sigma <- attr(scale(social[,-c(1)]),"scaled:scale")
```
Based on the plot, we decided to choose $k=7$.  We can now visualize the different clusters in the 2D PC space we chose earlier.  There seems to be a clear separation between clusters - those with a higher positive value for PC3 but low positive value for PC4 are individuals who love fitness (cluster 5).  Those who (we suspect) are college students are going to be in the positive PC4 range with negative PC3 values (cluster 4).
```{r}
set.seed(1)
cluster <- kmeans(scaled, 7, nstart=25)
scaled.points$cols <- cluster$cluster

qplot(PC3,PC4,data = scaled.points,color = factor(cols)) + 
  geom_point()
```
A closer examination of the clusters as determined by kmeans gives us a better picture for what type of followers are within this group.
```{r}
clust4 <- as.data.frame(sort((cluster$center[4,]*sigma + mu), decreasing=TRUE))
clust4$category <- rownames(clust4)
clust5 <- as.data.frame(sort((cluster$center[5,]*sigma + mu), decreasing=TRUE))
clust5$category <- rownames(clust5)
kmeansDF <- merge(clust4,clust5, by = "category")
colnames(kmeansDF)<- c("category","Cluster_4","Cluster_5")
ggplot(kmeansDF, aes(x=reorder(category,Cluster_4), y=Cluster_4)) +
  geom_bar(stat='identity') + coord_flip()
ggplot(kmeansDF, aes(x=reorder(category,Cluster_5), y=Cluster_5)) +
  geom_bar(stat='identity') + coord_flip()
```


## add stuff later


**Problem 5: 



setwd("C:/Users/timot/Documents/GitHub/PMAssignment/")
library(tidyverse)
library(tm)
library(slam)
library(proxy)

readerPlain = function(fname){
  readPlain(elem=list(content=readLines(fname)), 
            id=fname, language='en') }


file_list <- Sys.glob("ReutersC50/C50train/*")

total <- c()
author <- c()

for (i in 1:length(file_list)){
  articles <- Sys.glob(as.character(paste(file_list[i],"/*.txt", sep = "")))
  author <- c(author,rep(strsplit(file_list[i],"/")[[1]][3],length(articles)))
  total <- c(total,articles)
}

data <- cbind(author,total)
data <- cbind(data,lapply(data[,2],readerPlain))


documents_raw <- Corpus(VectorSource(data[,3]))

my_documents <- documents_raw %>%
  tm_map(content_transformer(tolower))  %>%             
  tm_map(content_transformer(removeNumbers)) %>%        
  tm_map(content_transformer(removePunctuation)) %>%    
  tm_map(content_transformer(stripWhitespace))          

my_documents <- tm_map(my_documents, content_transformer(removeWords), stopwords("en"))

DTM_train <- DocumentTermMatrix(my_documents)
inspect(DTM_train[1:10,1:20])

DTM_train <- removeSparseTerms(DTM_train, 0.95)
tfidf_train = weightTfIdf(DTM_train)

# TEST SET

file_list2 <- Sys.glob("ReutersC50/C50test/*")

total2 <- c()
author2 <- c()

for (i in 1:length(file_list2)){
  articles2 <- Sys.glob(as.character(paste(file_list2[i],"/*.txt", sep = "")))
  author2 <- c(author2,rep(strsplit(file_list2[i],"/")[[1]][3],length(articles2)))
  total2 <- c(total2,articles2)
}

data2 <- cbind(author2,total2)
data2 <- cbind(data2,lapply(data2[,2],readerPlain))


documents_raw2 <- Corpus(VectorSource(data2[,3]))

my_documents2 <- documents_raw2 %>%
  tm_map(content_transformer(tolower))  %>%             
  tm_map(content_transformer(removeNumbers)) %>%        
  tm_map(content_transformer(removePunctuation)) %>%    
  tm_map(content_transformer(stripWhitespace))          

my_documents2 <- tm_map(my_documents2, content_transformer(removeWords), stopwords("en"))

DTM_test <- DocumentTermMatrix(my_documents2)
inspect(DTM_test[1:10,1:20])

DTM_test <- removeSparseTerms(DTM_test, 0.95)
tfidf_test <- weightTfIdf(DTM_test)

# FILTER

X_train <- as.matrix(tfidf_train)
scrub <- which(colSums(X_train) == 0)
X_train <- X_train[,-scrub]

X_test <- as.matrix(tfidf_test)
scrub2 <- which(colSums(X_test) == 0)
X_test <- X_test[,-scrub2]


# MATCH

train.cols <- colnames(X_train)
test.cols <- colnames(X_test)

match <- intersect(train.cols,test.cols)
X_test <- X_test[,match]
X_train <- X_train[,match]


# DIMENSION REDUCTION

pca.x <- prcomp(X_train,scale=T)
plot(summary(pca.x)$importance[3,])

train <- pca.x$x[,1:400]
test <- predict(pca.x,newdata = X_test)[,1:400]

# PREDICTION

train <- as.data.frame(cbind(author,train))
test <- as.data.frame(cbind(author2,test))
colnames(test)[length(test)] <- "author"

library(randomForest)

set.seed(1)
rf <- randomForest(as.factor(author)~.,data = train,importance = T,mtry = 5,ntree =100)

preds <- predict(rf,newdata = test)
tab <- table(preds,as.factor(test$author))
totalsum <- 0
for (i in 1:dim(tab)[1]){
  totalsum <- totalsum + tab[i,i]
}



# KNN

library(kknn)

tr <- train
t <- test

knn <- kknn(as.factor(author)~.,train = tr, test = t, k = 50)




# Boosting

#library(gbm)

#boosted <- gbm(as.factor(author)~.,data = train)
